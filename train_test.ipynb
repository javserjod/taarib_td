{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from tqdm import tqdm   # show loops progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA version from PyTorch: {torch.version.cuda}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SEED**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED:int = 42\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LOAD TRAIN DATA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset personalizado para DRIVE\n",
    "class Dataset(Dataset):\n",
    "    def __init__(self, image_dir, ground_truth, transform=None, transform_img=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.ground_truth_dir = ground_truth\n",
    "        self.image_filenames = os.listdir(image_dir)  \n",
    "        self.mask_filenames = os.listdir(ground_truth)   \n",
    "        self.transform = transform\n",
    "        self.transform_img = transform_img\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = os.path.join(self.image_dir, self.image_filenames[idx])\n",
    "        ground_truth_path = os.path.join(self.ground_truth_dir, self.mask_filenames[idx])\n",
    "        \n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        ground_truth = Image.open(ground_truth_path).convert('L')  # gt to grayscale\n",
    "        \n",
    "        # image to tensor (needed to concatenate and more)\n",
    "        image = transforms.ToTensor()(image)\n",
    "        ground_truth = transforms.ToTensor()(ground_truth)\n",
    "        \n",
    "        if self.transform:        # exactly same transformations to image and gt\n",
    "            # Concatenate along channel dimension.\n",
    "            # Here, dim=0 is the channel dimension (not the batch dim) (here we have [C, H, W])\n",
    "            image_and_gt = torch.cat([image, ground_truth], dim=0) \n",
    "            \n",
    "            # Transform together\n",
    "            transformed = self.transform(image_and_gt)\n",
    "\n",
    "            # Slice the tensors out\n",
    "            image = transformed[:3, :, :]    \n",
    "            ground_truth = (transformed[3:, :, :] > 0.5).float()       # binarize !!!\n",
    "        \n",
    "        \n",
    "        if self.transform_img:       # transformations only for images\n",
    "            image = self.transform_img(image)\n",
    "            \n",
    "\n",
    "        return image, ground_truth, self.image_filenames[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Routes for training data\n",
    "image_dir = './data/CHASE/training/images'\n",
    "ground_truth_dir = './data/CHASE/training/1st_manual'\n",
    "\n",
    "''' # Just compute the mean and std\n",
    "from torchvision import datasets\n",
    "# calculate mean and std\n",
    "dataset = Dataset(image_dir, ground_truth_dir, transform=transforms.ToTensor())\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "NORM_MEAN = 0.\n",
    "NORM_STD = 0.\n",
    "for images, _, _ in dataloader:\n",
    "    NORM_MEAN += images.mean([0, 2, 3])  # channels means (R, G, B)\n",
    "    NORM_STD += images.std([0, 2, 3])    # channels std\n",
    "\n",
    "NORM_MEAN /= len(dataloader)\n",
    "NORM_STD /= len(dataloader)\n",
    "\n",
    "print(f\"Mean: {NORM_MEAN}\")\n",
    "print(f\"Std: {NORM_STD}\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE:int = 512\n",
    "DATA_AUG_PROB:float = 0.75\n",
    "\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "# transforms for images and its ground truth segmentations (same for both)\n",
    "transform = v2.Compose([\n",
    "    v2.RandomApply([\n",
    "        v2.RandomRotation(degrees=(-180, 180)),\n",
    "        v2.RandomHorizontalFlip(),\n",
    "        v2.RandomVerticalFlip(),\n",
    "        v2.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 5.)),\n",
    "        v2.RandomAffine(degrees=(30, 70), translate=(0.1, 0.3), scale=(0.5, 0.75)),\n",
    "        v2.RandomResizedCrop(IMG_SIZE),\n",
    "        #v2.RandomZoomOut(fill=1)  \n",
    "    ], p=DATA_AUG_PROB),\n",
    "    v2.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    \n",
    "    # transforms.Grayscale(num_output_channels=1),       # we want rgb    \n",
    "    #transforms.ToTensor(),             # already tensors\n",
    "    #transforms.Normalize(mean=NORM_MEAN, std=NORM_STD),   # no\n",
    "    \n",
    "])\n",
    "\n",
    "# transform just for the image\n",
    "transform_img = v2.Compose([\n",
    "    v2.RandomApply([\n",
    "        v2.ColorJitter(brightness=[0.6, 1.2], contrast=[0.5, 1.2], saturation=[0.6, 1.2], hue=0.015),\n",
    "        v2.RandomAdjustSharpness(sharpness_factor=2),\n",
    "        \n",
    "    ], p=DATA_AUG_PROB),\n",
    "     \n",
    "    #transforms.ToTensor(),             # already tensors\n",
    "    #transforms.Normalize(mean=NORM_MEAN, std=NORM_STD),   # no\n",
    "    \n",
    "])\n",
    "\n",
    "\n",
    "dataset = Dataset(image_dir, ground_truth_dir, transform=transform, transform_img=transform_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_loader = DataLoader(dataset, batch_size=8, shuffle=True, num_workers=0)\n",
    "for img, gt, _ in example_loader:\n",
    "    print(img.shape)\n",
    "    print(gt.shape)\n",
    "    print(\"\\n\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_loader = DataLoader(dataset, batch_size=8, shuffle=True, num_workers=0)\n",
    "\n",
    "# Obtener algunas imágenes y máscaras del example_loader\n",
    "data_iter = iter(example_loader)\n",
    "images, ground_truth, img_name = next(data_iter)\n",
    "\n",
    "# Convertir los tensores a formato numpy para visualizarlos\n",
    "images = images.numpy().transpose(0, 2, 3, 1)  # [N, C, H, W] a [N, H, W, C]\n",
    "ground_truth = ground_truth.numpy()  # [N, H, W] para las máscaras\n",
    "\n",
    "# Eliminar la dimensión extra (1, H, W) de las máscaras\n",
    "ground_truth = np.squeeze(ground_truth)  # Esto convierte la forma (1, H, W) a (H, W)\n",
    "\n",
    "# Visualizar las imágenes en la fila superior y las máscaras en la fila inferior\n",
    "fig, axes = plt.subplots(2, 4, figsize=(15, 7))  # 2 filas, 4 columnas (imágenes arriba y máscaras abajo)\n",
    "\n",
    "for i in range(4):\n",
    "    # Mostrar la imagen en la fila superior\n",
    "    axes[0, i].imshow(images[i])\n",
    "    axes[0, i].axis('off')\n",
    "    axes[0, i].set_title(f'Original {img_name[i]}')\n",
    "    \n",
    "    # Mostrar la máscara en la fila inferior\n",
    "    axes[1, i].imshow(ground_truth[i], cmap='gray')  # show as grayscale\n",
    "    axes[1, i].axis('off')\n",
    "    axes[1, i].set_title(f'GT Segmentation {img_name[i]}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DATA SPLITTING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VAL_PERCENT:float = 0.2            # Percentage of dataset intended for validation (rest is for training) \n",
    "BATCH_SIZE:int = 4\n",
    "\n",
    "# Split into train / validation partitions\n",
    "val_size = int(len(dataset) * VAL_PERCENT)     # number of samples for validation\n",
    "train_size = len(dataset) - val_size           # number of samples for training\n",
    "train_set, val_set = random_split(dataset, [train_size, val_size], generator=torch.Generator().manual_seed(SEED))\n",
    "\n",
    "# Create data loaders for our datasets; shuffle for training, not for validation\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print('Training set has {} instances'.format(train_size))\n",
    "print('Validation set has {} instances'.format(val_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_image, sample_gt, _ = train_set[0]\n",
    "\n",
    "NUM_CHANNELS_IN:int = sample_image.size(0)  \n",
    "NUM_CHANNELS_OUT:int = sample_gt.size(0)\n",
    "\n",
    "print(f\"Number of channels in input: {NUM_CHANNELS_IN}\")\n",
    "print(f\"Number of channels in output: {NUM_CHANNELS_OUT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CREATE MODEL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''from unet import UNet\n",
    "model = UNet(n_channels=NUM_CHANNELS_IN, n_classes=NUM_CHANNELS_OUT)'''\n",
    "\n",
    "from vig import Classifier\n",
    "NUM_PATCHES:int = 196    #196\n",
    "PATCH_SIZE:int = int(IMG_SIZE//np.sqrt(NUM_PATCHES))\n",
    "print(f\"{PATCH_SIZE=}\")\n",
    "\n",
    "model = Classifier(in_features = NUM_CHANNELS_IN * PATCH_SIZE * PATCH_SIZE,       # size of each input patch\n",
    "                   n_classes = NUM_CHANNELS_OUT,                                  # number of classes to predict\n",
    "                   num_patches = NUM_PATCHES,                                      \n",
    "                   patch_size = PATCH_SIZE,\n",
    "                   output_size = IMG_SIZE,\n",
    "                   out_feature = 192,                                             # features per node/patch (tiny: 192 |small: 320 |base: 640)\n",
    "                   num_ViGBlocks=16,                                              # number of Grapher module + FFN (tiny: 12 |small: 16 |base: 16)\n",
    "                   hidden_layer=1024,\n",
    "                   num_edges=9,                                                   # number of neigbours per node\n",
    "                   head_num=1,\n",
    "                  )\n",
    "\n",
    "# cuda if available, cpu if not\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"{device.type=}\")\n",
    "model.to(device)      # move to cuda if possible\n",
    "\n",
    "from torchsummary import summary\n",
    "#summary(model, (NUM_CHANNELS_IN, IMG_SIZE, IMG_SIZE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SOME HYPERPARAMETERS, SAVE PATH, CUDA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS:int = 200\n",
    "\n",
    "# save best model state path\n",
    "SAVE_PATH = \"./trained_models\"\n",
    "       \n",
    "\n",
    "# LOSS FUNCTION -----------------------------------------------------\n",
    "from utils.loss_functions import DiceLoss, DiceBCELoss\n",
    "#(need sigmoid at the end of network)\n",
    "\n",
    "#loss_fn = torch.nn.BCELoss()        # binary cross-entropy loss \n",
    "#loss_fn = DiceLoss()\n",
    "loss_fn = DiceBCELoss()\n",
    "\n",
    "\n",
    "\n",
    "# OPTIMIZER ---------------------------------------------------------\n",
    "LEARNING_RATE:float = 0.001\n",
    "MOMENTUM:float = 0.9\n",
    "WEIGHT_DECAY:float = 1e-2\n",
    "\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TRAIN FUNCTION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(epochs, model, train_dataloader, val_dataloader, \n",
    "                  loss_fn, optimizer, save_path):\n",
    "    \n",
    "    history = {'train_loss': [], 'val_loss': [], 'train_accuracy':[], 'val_accuracy':[]}\n",
    "    best_val_loss = float('inf')  # Initialize to infinity\n",
    "    best_model_state = None\n",
    "    datetime_start = datetime.now()\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        model.train()\n",
    "        running_train_loss = 0\n",
    "        train_correct_pred = 0\n",
    "        total_train_pred = 0\n",
    "        \n",
    "        for data in tqdm(train_dataloader):      # for each training batch\n",
    "            \n",
    "            img, ground_truth, _ = data\n",
    "            img, ground_truth = img.to(device), ground_truth.to(device)     # move to cuda, if possible\n",
    "            \n",
    "            pred = model(img)\n",
    "            # print(f\"Pred shape: {pred.shape}, Ground truth shape: {ground_truth.shape}\")   # debug\n",
    "            # print(torch.max(pred))\n",
    "            # print(torch.min(pred))\n",
    "            # print(f\"Pred device: {pred.device}, Ground truth device: {ground_truth.device}\")\n",
    "\n",
    "            # print(f\"Ground truth values: {ground_truth.unique()}\")\n",
    "            # print(\"\\n\")\n",
    "\n",
    "            loss = loss_fn(pred, ground_truth)\n",
    "            # print(loss)\n",
    "            # print(loss.shape)\n",
    "            running_train_loss += loss.item()\n",
    "            train_correct_pred += ((pred > 0.5).float() == ground_truth).sum()      # add number of correct predictions\n",
    "            total_train_pred += IMG_SIZE*IMG_SIZE*len(img)            # add number of predictions made (pixels per img * number of imgs)\n",
    "            \n",
    "            loss.backward()            # calculate gradients\n",
    "            optimizer.step()           # update model parameters\n",
    "            optimizer.zero_grad()      # reset gradients\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            running_val_loss = 0\n",
    "            val_correct_pred = 0\n",
    "            total_val_pred = 0\n",
    "            \n",
    "            for data in tqdm(val_dataloader):      # for each validation batch\n",
    "                \n",
    "                img, ground_truth, _ = data\n",
    "                img, ground_truth = img.to(device), ground_truth.to(device)       # move to cuda, if possible\n",
    "                \n",
    "                pred = model(img)\n",
    "                loss = loss_fn(pred, ground_truth)\n",
    "                \n",
    "                running_val_loss += loss.item()\n",
    "                val_correct_pred += ((pred > 0.5).float() == ground_truth).sum()         # add number of correct predictions\n",
    "                total_val_pred += IMG_SIZE*IMG_SIZE*len(img)                     # add number of predictions made (pixels per img * number of imgs)\n",
    "\n",
    "\n",
    "        train_loss = running_train_loss / len(train_dataloader.dataset)\n",
    "        val_loss = running_val_loss / len(val_dataloader.dataset)\n",
    "        train_acc = train_correct_pred / total_train_pred * 100       # % of average accuracy in training, in this epoch\n",
    "        val_acc = val_correct_pred / total_val_pred * 100             # % of average accuracy in validation, in this epoch\n",
    "\n",
    "        history['train_loss'].append(train_loss)         # add metrics in this epoch to the record\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['train_accuracy'].append(train_acc)\n",
    "        history['val_accuracy'].append(val_acc)\n",
    "        \n",
    "        print(f'Epoch: {epoch}/{epochs} | Training loss: {train_loss:.5f} | Validation loss: {val_loss:.5f} | Training accuracy: {train_acc:.2f}% | Validation accuracy: {val_acc:.2f}%')\n",
    "        print(f\"\\n\")\n",
    "        \n",
    "        # Save the model if the validation loss improved\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_state = model.state_dict()  # Save the model state\n",
    "\n",
    "    # Save the best model state\n",
    "    if best_model_state is not None:\n",
    "        model_path = save_path + '/' + '{}_{}_{:.5f}'.format(datetime_start.strftime('%Y%m%d_%H%M%S'), type(model).__name__, best_val_loss)\n",
    "        torch.save(best_model_state, model_path)    \n",
    "        print(f'Best model saved at {model_path}')\n",
    "    \n",
    "    # save history (dictionary)\n",
    "    history_path = save_path + '/' + '{}_{}_{:.5f}'.format(datetime_start.strftime('%Y%m%d_%H%M%S'), type(model).__name__, best_val_loss)+'_history'\n",
    "    torch.save(history, history_path)\n",
    "    print(f'\\nHistory saved at {history_path}')\n",
    "    \n",
    "    model.eval()\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN THE CURRENT MODEL\n",
    "start_time = datetime.now()\n",
    "\n",
    "history = training_loop(EPOCHS, model, train_loader, val_loader, \n",
    "                        loss_fn, optimizer, SAVE_PATH)\n",
    "\n",
    "end_time = datetime.now()\n",
    "print(f\"\\nTraining duration: {end_time - start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a previously saved history can be loaded\n",
    "#history = torch.load(\"./trained_models/20250121_200549_UNet_0.02112_history\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(max(10, 0.05 * EPOCHS), 7))\n",
    "plt.plot(np.arange(1, EPOCHS+1), history['train_loss'], label='Training loss')\n",
    "plt.plot(np.arange(1, EPOCHS+1), history['val_loss'], label='Validation loss')\n",
    "\n",
    "min_val_loss = min(history['val_loss'])\n",
    "min_val_loss_epoch = history['val_loss'].index(min_val_loss) +1\n",
    "plt.axvline(x=min_val_loss_epoch, color='red', linestyle='--', label=f'Min Val Loss ({min_val_loss:.4f}) at epoch {min_val_loss_epoch} [saved]')\n",
    "\n",
    "interval=10\n",
    "ticks = [1] + list(range(interval, EPOCHS + 1, interval))   #start in 1, go 1, 10, 20, 30...\n",
    "plt.xticks(ticks)\n",
    "#fig.autofmt_xdate()\n",
    "\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensor to numpy\n",
    "train_accuracy_cpu = [acc.cpu().numpy()/100 for acc in history['train_accuracy']]\n",
    "val_accuracy_cpu = [acc.cpu().numpy()/100 for acc in history['val_accuracy']]\n",
    "#print(train_accuracy_cpu)\n",
    "\n",
    "fig = plt.figure(figsize=(max(10, 0.05 * EPOCHS), 7))\n",
    "plt.plot(np.arange(1,EPOCHS+1), train_accuracy_cpu, label='Training accuracy')\n",
    "plt.plot(np.arange(1,EPOCHS+1), val_accuracy_cpu, label='Validation accuracy')\n",
    "\n",
    "max_val_acc = max(history['val_accuracy'])\n",
    "max_val_acc_epoch = history['val_accuracy'].index(max_val_acc) +1\n",
    "plt.axvline(x=max_val_acc_epoch, color='red', linestyle='--', label=f'Max Val Acc ({max_val_acc:.2f}%) at epoch {max_val_acc_epoch}')\n",
    "\n",
    "interval=10\n",
    "ticks = [1] + list(range(interval, EPOCHS + 1, interval))   #start in 1, go 1, 10, 20, 30...\n",
    "plt.xticks(ticks)\n",
    "\n",
    "#fig.autofmt_xdate()\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TEST**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create model and load saved state \n",
    "'''model = UNet(n_channels=NUM_CHANNELS_IN, n_classes=NUM_CHANNELS_OUT)\n",
    "load_model_path = './trained_models/20250122_004444_UNet_0.04689'\n",
    "model.load_state_dict(torch.load(load_model_path))'''\n",
    "model.to(device)\n",
    "model.eval()     # !!! set the dropout and batch normalization layers to evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((512, 512)),  # evaluation at native resolution\n",
    "    #transforms.ToTensor(),    # already tensor\n",
    "    #transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), \n",
    "])\n",
    "\n",
    "# load test images\n",
    "test_img_dir = './data/CHASE/test/images'\n",
    "test_gt_dir = './data/CHASE/test/1st_manual'\n",
    "\n",
    "test_dataset = Dataset(test_img_dir, test_gt_dir, transform=test_transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check test image\n",
    "for img, gt, img_name in test_loader:\n",
    "    print(f\"Shape of image: {img.shape}\")\n",
    "    print(f\"Shape of GT: {gt.shape}\")\n",
    "    print(f\"Name of image: {img_name[0]}\") \n",
    "    break  # just one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()   # done before, but just in case...\n",
    "\n",
    "num_images = 4  # number of images to show\n",
    "fig, axes = plt.subplots(3, num_images, figsize=(10, 7))\n",
    "\n",
    "i = 0  # counter\n",
    "\n",
    "with torch.no_grad():  # no gradient needed\n",
    "    for image, gt, image_name in test_loader:\n",
    "        if i >= num_images: \n",
    "            break\n",
    "\n",
    "        # move to cuda\n",
    "        image = image.to(device)\n",
    "\n",
    "        output = model(image)  # predict\n",
    "        output_binary = (output > 0.5).float()  # convert to 0 or 1\n",
    "\n",
    "        # convert output to image\n",
    "        output_img = output_binary.squeeze().cpu().numpy()  # delete batch dimension\n",
    "        gt_img = gt.squeeze().cpu().numpy()\n",
    "\n",
    "        # original test images\n",
    "        axes[0, i].imshow(image.squeeze().cpu().numpy().transpose(1, 2, 0))  # from tensor to image\n",
    "        axes[0, i].axis('off')\n",
    "        axes[0, i].set_title(f\"Original: {image_name[0]}\")\n",
    "        \n",
    "        # test images ground truth segmentations\n",
    "        axes[1, i].imshow(gt_img, cmap='gray')  # grayscale\n",
    "        axes[1, i].axis('off')\n",
    "        axes[1, i].set_title(f\"Ground Truth: {image_name[0]}\")\n",
    "\n",
    "        # test images predicted segmentations\n",
    "        axes[2, i].imshow(output_img, cmap='gray')  # grayscale\n",
    "        axes[2, i].axis('off')\n",
    "        axes[2, i].set_title(f\"Segmentation: {image_name[0]}\")\n",
    "\n",
    "        i += 1\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()   # done before, but just in case...\n",
    "\n",
    "num_images = 3   # number of images to show\n",
    "fig, axes = plt.subplots(1, num_images, figsize=(12, 8))\n",
    "\n",
    "i = 0  # counter\n",
    "\n",
    "with torch.no_grad():  # no gradient needed\n",
    "    for image, gt, image_name in test_loader:\n",
    "        if i >= num_images: \n",
    "            break\n",
    "\n",
    "        # move to CUDA\n",
    "        image = image.to(device)\n",
    "\n",
    "        output = model(image)  # predict\n",
    "        output_binary = (output > 0.5).float()  # binarize prediction\n",
    "\n",
    "        # convert prediction to image\n",
    "        output_img = output_binary.squeeze().cpu().numpy()  # delete batch dimension\n",
    "        gt_img = gt.squeeze().cpu().numpy()  # gt as image\n",
    "\n",
    "        # Mostrar ground truth como fondo en escala de grises\n",
    "        axes[i].imshow(gt_img, cmap='gray')  # Fondo: ground truth\n",
    "        \n",
    "        # Crear imagen RGBA para superponer\n",
    "        overlay = np.zeros((*output_img.shape, 4))  # Crear un mapa RGBA\n",
    "        overlay[..., 0] = 1  # Canal rojo\n",
    "        overlay[..., 3] = output_img  # Canal alfa (0 para transparente, 1 para opaco)\n",
    "        \n",
    "        axes[i].imshow(overlay)  # Superposición en RGBA\n",
    "        \n",
    "        axes[i].axis('off')\n",
    "        axes[i].set_title(f\"Overlay: {image_name[0]}\")\n",
    "\n",
    "        i += 1\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ROC, AUC, Test Accuracy, Dice score, MCC, ccDice**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc, accuracy_score, f1_score\n",
    "from utils.dice_score import dice_score                     # custom function to calculate dice score\n",
    "from utils.matthews_corr_coef import matthews_corr_coef     # custom function to calculate MCC\n",
    "from utils.ccDice_score import ccDice                       # modified ccDice score (only binary labels)\n",
    "from skimage.measure import label                           # necessary for ccDice\n",
    "\n",
    "calculate_ccdice = True    # yes or no. Forced to use CPU: it takes long to compute (5 minutes just 8 images)\n",
    "\n",
    "all_labels = []       # to store ground truth values for each pixel of each image\n",
    "all_preds = []        # to store predicted values for each pixel of each image\n",
    "\n",
    "dice_score_batches = []       # to store Dice score of each batch\n",
    "mcc_batches = []              # to store MCC of each batch\n",
    "if calculate_ccdice==True:\n",
    "    ccdice_score_batches = []     # to store ccDice score of each batch\n",
    "\n",
    "with torch.no_grad():  # no gradients needed\n",
    "    for inputs, ground_truth, _ in test_loader:        # for each batch in test loader\n",
    "        inputs = inputs.to(device)\n",
    "        ground_truth = ground_truth.to(device)\n",
    "\n",
    "        outputs = model(inputs)     # predict\n",
    "\n",
    "        all_labels.append(ground_truth.cpu().numpy())       # real labels for each pixel (0 or 1) \n",
    "        all_preds.append(outputs.cpu().numpy())             # probabilities to belong to class 1, given by model\n",
    "        \n",
    "        dice = dice_score(outputs, ground_truth, threshold=0.5)\n",
    "        dice_score_batches.append(dice.item())\n",
    "        \n",
    "        mcc = matthews_corr_coef(outputs, ground_truth, threshold=0.5)\n",
    "        mcc_batches.append(mcc)\n",
    "\n",
    "        if calculate_ccdice==True:\n",
    "            y_pred_label, cc_pred = label(((outputs > 0.5).float()).cpu().numpy().squeeze(0), return_num=True)\n",
    "            y_true_label, cc_true = label(ground_truth.cpu().numpy().squeeze(0), return_num=True)\n",
    "            ccdice = ccDice(y_pred_label, cc_pred, y_true_label, cc_true, alpha=0.1)    # alpha is level of exigency to join 2 cc (low is permissive, 1.0 is max exigency)\n",
    "            ccdice_score_batches.append(ccdice)\n",
    "\n",
    "# compute mean dice score and print\n",
    "mean_dice_score = torch.tensor(dice_score_batches).mean()\n",
    "print(f\"\\nDice score: {mean_dice_score:.4f}\")\n",
    "\n",
    "# compute mean MCC and print\n",
    "mean_mcc = torch.tensor(mcc_batches).mean()\n",
    "print(f\"\\nMCC: {mean_mcc:.4f}\")\n",
    "\n",
    "# compute mean ccDice score and print\n",
    "if calculate_ccdice==True:\n",
    "    mean_ccdice_score = torch.tensor(ccdice_score_batches).mean()\n",
    "    print(f\"\\nccDice score: {mean_ccdice_score:.4f}\")\n",
    "\n",
    "# list to numpy array\n",
    "all_labels = np.concatenate(all_labels, axis=0)\n",
    "all_preds = np.concatenate(all_preds, axis=0)\n",
    "\n",
    "# compute mean test accuracy and print\n",
    "accuracy_sklearn = accuracy_score(list(map(bool,all_labels.flatten())), (all_preds.flatten() > 0.5))\n",
    "print(f\"\\nTest Accuracy: {accuracy_sklearn:.4f}\")\n",
    "\n",
    "# compute mean F1 score accuracy and print\n",
    "f1_sklearn = f1_score(list(map(bool,all_labels.flatten())), (all_preds.flatten() > 0.5))\n",
    "print(f\"\\nF1 Score: {f1_sklearn:.4f}\")\n",
    "\n",
    "# ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(list(map(bool,all_labels.flatten())), all_preds.flatten())     # force labels to binary list (if not, error when transform.resize)\n",
    "\n",
    "# AUC\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# plot ROC\n",
    "print(\"\\n\")\n",
    "plt.figure(figsize=(7, 7))\n",
    "plt.plot(fpr, tpr, color='blue', label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', linestyle='--') \n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just to visualize the model architecture\n",
    "\n",
    "'''from torch.utils.tensorboard import SummaryWriter\n",
    "import torchvision\n",
    "\n",
    "# default `log_dir` is \"runs\" - we'll be more specific here\n",
    "writer = SummaryWriter('runs/UNet')\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(train_loader)\n",
    "images, labels, _ = next(dataiter)\n",
    "\n",
    "print(images.shape)\n",
    "images = images.to(device)\n",
    "\n",
    "# create grid of images\n",
    "img_grid = torchvision.utils.make_grid(images)\n",
    "\n",
    "#img_grid = img_grid.unsqueeze(0) \n",
    "\n",
    "# write to tensorboard\n",
    "writer.add_image('train_img', img_grid)\n",
    "\n",
    "writer.add_graph(model, images)\n",
    "writer.close()\n",
    "\n",
    "# write in terminal --->   tensorboard --logdir=runs/Unet'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
