{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "from pathlib import Path\n",
    "from torch import optim\n",
    "\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# PyTorch TensorBoard support\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "\n",
    "from torchvision.datasets import ImageFolder\n",
    "from tqdm import tqdm   # show loops progress\n",
    "\n",
    "#import wandb    # track and visualize aspects of training proccess in real time\n",
    "#import evaluate\n",
    "from unet import UNet\n",
    "#from utils.data_loading import BasicDataset, CarvanaDataset\n",
    "from utils.dice_score import dice_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1\n",
      "CUDA version from PyTorch: 12.1\n"
     ]
    }
   ],
   "source": [
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA version from PyTorch: {torch.version.cuda}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Carga de datos de entrenamiento**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset personalizado para DRIVE\n",
    "class DriveDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.image_filenames = sorted(os.listdir(image_dir))  # Ordenar para emparejar correctamente\n",
    "        self.mask_filenames = sorted(os.listdir(mask_dir))    # Ordenar para emparejar correctamente\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = os.path.join(self.image_dir, self.image_filenames[idx])\n",
    "        mask_path = os.path.join(self.mask_dir, self.mask_filenames[idx])\n",
    "        \n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        mask = Image.open(mask_path).convert('L')  # Convertir a escala de grises para las máscaras\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            mask = self.transform(mask)\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "\n",
    "IMG_SIZE = 512\n",
    "\n",
    "# Transformaciones para las imágenes y las máscaras\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((512, 512)),  # Ajusta según tus necesidades\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Routes\n",
    "image_dir = './data/DRIVE/training/images'\n",
    "gt_dir = './data/DRIVE/training/1st_manual'\n",
    "#dir_checkpoint = Path('./checkpoints/')        # donde se guardarán\n",
    "\n",
    "dataset = DriveDataset(image_dir, gt_dir, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_loader = DataLoader(dataset, batch_size=8, shuffle=True, num_workers=0)\n",
    "\n",
    "# Obtener algunas imágenes y máscaras del example_loader\n",
    "data_iter = iter(example_loader)\n",
    "images, masks = next(data_iter)\n",
    "\n",
    "# Convertir los tensores a formato numpy para visualizarlos\n",
    "images = images.numpy().transpose(0, 2, 3, 1)  # [N, C, H, W] a [N, H, W, C]\n",
    "masks = masks.numpy()  # [N, H, W] para las máscaras\n",
    "\n",
    "# Eliminar la dimensión extra (1, H, W) de las máscaras\n",
    "masks = np.squeeze(masks)  # Esto convierte la forma (1, H, W) a (H, W)\n",
    "\n",
    "# Visualizar las imágenes en la fila superior y las máscaras en la fila inferior\n",
    "fig, axes = plt.subplots(2, 4, figsize=(15, 7))  # 2 filas, 4 columnas (imágenes arriba y máscaras abajo)\n",
    "\n",
    "for i in range(4):\n",
    "    # Mostrar la imagen en la fila superior\n",
    "    axes[0, i].imshow(images[i])\n",
    "    axes[0, i].axis('off')\n",
    "    axes[0, i].set_title(f'Imagen {i+1}')\n",
    "    \n",
    "    # Mostrar la máscara en la fila inferior\n",
    "    axes[1, i].imshow(masks[i], cmap='gray')  # Usar escala de grises para las máscaras\n",
    "    axes[1, i].axis('off')\n",
    "    axes[1, i].set_title(f'Máscara {i+1}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VAL_PERCENT:float = 0.2            # Percentage of dataset intended for validation (rest is for training) \n",
    "BATCH_SIZE:int = 4\n",
    "\n",
    "# Split into train / validation partitions\n",
    "val_size = int(len(dataset) * VAL_PERCENT)     # number of samples for validation\n",
    "train_size = len(dataset) - val_size           # number of samples for training\n",
    "train_set, val_set = random_split(dataset, [train_size, val_size], generator=torch.Generator().manual_seed(0))\n",
    "\n",
    "# Create data loaders for our datasets; shuffle for training, not for validation\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print('Training set has {} instances'.format(train_size))\n",
    "print('Validation set has {} instances'.format(val_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_image, sample_label = train_set[0]\n",
    "\n",
    "NUM_CHANNELS_IN:int = sample_image.size(0)  \n",
    "NUM_CHANNELS_OUT:int = sample_label.size(0)\n",
    "\n",
    "print(f\"Number of channels in input: {NUM_CHANNELS_IN}\")\n",
    "print(f\"Number of channels in output: {NUM_CHANNELS_OUT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS:int = 100\n",
    "LEARNING_RATE:float = 0.001\n",
    "\n",
    "SAVE_PATH = \"./trained_models\"        # where to save the trained best model state\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"{device.type=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unet import UNet\n",
    "model = UNet(n_channels=NUM_CHANNELS_IN, n_classes=NUM_CHANNELS_OUT)\n",
    "model.to(device)      # move to cuda if possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "summary(model, (NUM_CHANNELS_IN, IMG_SIZE, IMG_SIZE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TRAIN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(epochs, model, train_dataloader, val_dataloader, \n",
    "                  loss_fn, optimizer, save_path):\n",
    "    \n",
    "    history = {'train_loss': [], 'val_loss': []}\n",
    "    best_val_loss = float('inf')  # Initialize to infinity\n",
    "    best_model_state = None\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        model.train()\n",
    "        running_train_loss = 0\n",
    "        for data in tqdm(train_dataloader):\n",
    "            img, mask = data\n",
    "            img, mask = img.to(device), mask.to(device)\n",
    "            pred = model(img)\n",
    "            loss = loss_fn(pred, mask)\n",
    "            running_train_loss += loss.item()\n",
    "            loss.backward()    # calculate gradients\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            running_val_loss = 0\n",
    "            for data in tqdm(val_dataloader):\n",
    "                img, mask = data\n",
    "                img, mask = img.to(device), mask.to(device)\n",
    "                pred = model(img)\n",
    "                loss = loss_fn(pred, mask)\n",
    "                running_val_loss += loss.item()\n",
    "\n",
    "        train_loss = running_train_loss / len(train_dataloader.dataset)\n",
    "        val_loss = running_val_loss / len(val_dataloader.dataset)\n",
    "\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        print(f'Epoch: {epoch}/{epochs} | Training loss: {train_loss} | Validation loss: {val_loss}')\n",
    "        \n",
    "        # Save the model if the validation loss improved\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_state = model.state_dict()  # Save the model state\n",
    "\n",
    "    # Save the best model state\n",
    "    if best_model_state is not None:\n",
    "        model_path = save_path+'/'+'model_{}_{}'.format(type(model).__name__, datetime.now().strftime('%Y%m%d_%H%M%S'))\n",
    "        torch.save(best_model_state, model_path)\n",
    "        print(f'\\nBest model saved at {model_path}')\n",
    "    \n",
    "    model.eval()\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = training_loop(EPOCHS, model, train_loader, val_loader, \n",
    "                        loss_fn, optimizer, SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7, 7))\n",
    "plt.plot(np.arange(1,EPOCHS+1), history['train_loss'], label='Training loss')\n",
    "plt.plot(np.arange(1,EPOCHS+1), history['val_loss'], label='Validation loss')\n",
    "interval = 10 \n",
    "plt.xticks(np.arange(1, EPOCHS + 1, interval))\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TEST**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet(n_channels=NUM_CHANNELS_IN, n_classes=NUM_CHANNELS_OUT)\n",
    "model.load_state_dict(torch.load('./trained_models/model_UNet_20250118_220040'))\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((512, 512)),  # Ajusta según tus necesidades\n",
    "    transforms.ToTensor(),\n",
    "    #transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), \n",
    "])\n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, image_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.image_names = os.listdir(image_dir)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.image_dir, self.image_names[idx])\n",
    "        image = Image.open(img_name).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, self.image_names[idx]  # return image and its name\n",
    "\n",
    "# load test images\n",
    "test_dir = './data/DRIVE/test/images'\n",
    "test_dataset = TestDataset(test_dir, transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for images, labels in test_loader:  # test_loader es tu DataLoader\n",
    "    print(f\"Shape of image: {images.shape}\")  # Forma de la imagen\n",
    "    break  # Solo mostrar el tamaño de la primera imagen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_images = 4  # number of images to show\n",
    "fig, axes = plt.subplots(2, num_images, figsize=(15, 7))\n",
    "\n",
    "i = 0  # counter\n",
    "\n",
    "with torch.no_grad():  # no gradient needed\n",
    "    for image, image_name in test_loader:\n",
    "        if i >= num_images: \n",
    "            break\n",
    "\n",
    "        # move to cuda\n",
    "        image = image.to(device)\n",
    "\n",
    "        output = model(image)  # predict\n",
    "        output = torch.sigmoid(output)  # apply sigmoid if mask binary\n",
    "        output = (output > 0.5).float()  # convert to 0 or 1\n",
    "\n",
    "        # convert output to image\n",
    "        output_img = output.squeeze().cpu().numpy()  # delete batch dimension\n",
    "\n",
    "        # original test images\n",
    "        axes[0, i].imshow(image.squeeze().cpu().numpy().transpose(1, 2, 0))  # from tensor to image\n",
    "        axes[0, i].axis('off')\n",
    "        axes[0, i].set_title(f\"Original: {image_name[0]}\")\n",
    "\n",
    "        # test images predicted segmentations\n",
    "        axes[1, i].imshow(output_img, cmap='gray')  # grayscale\n",
    "        axes[1, i].axis('off')\n",
    "        axes[1, i].set_title(f\"Segmentation: {image_name[0]}\")\n",
    "\n",
    "        i += 1\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
